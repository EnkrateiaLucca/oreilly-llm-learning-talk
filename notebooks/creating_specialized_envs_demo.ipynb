{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Using cached graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain\n",
    "# !pip install pydantic\n",
    "# !pip install instructor\n",
    "# !pip install openai\n",
    "# !pip install ipywidgets\n",
    "# !pip install chromadb\n",
    "# !pip install pypdf\n",
    "# !pip install tiktoken\n",
    "# !pip install ipywidgets\n",
    "# !pip install matplotlib\n",
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Over docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cell_1\n",
    "\n",
    "\"./assets-resources/docs/NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./assets-resources/docs/NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_1.outputs[0].data[\"text/plain\"].strip(\"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source for this code mostly from: https://towardsdatascience.com/run-interactive-sessions-with-chatgpt-in-jupyter-notebook-87e00f2ee461\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML, display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "def text_eventhandler(*args):\n",
    "    # Needed bc when we \"reset\" the text input\n",
    "    # it fires instantly another event since\n",
    "    # we \"changed\" it's value to \"\"\n",
    "    if args[0][\"new\"] == \"\":\n",
    "        return\n",
    "\n",
    "    # Show loading animation\n",
    "    loading_bar.layout.display = \"block\"\n",
    "\n",
    "    # Get question\n",
    "    question = args[0][\"new\"]\n",
    "\n",
    "    # Reset text field\n",
    "    args[0][\"owner\"].value = \"\"\n",
    "\n",
    "    # Formatting question for output\n",
    "    q = (\n",
    "        f'<div class=\"chat-message-right pb-4\"><div>'\n",
    "        + f'<img src=\"assets-resources/bear.png\" class=\"rounded-circle mr-1\" width=\"40\" height=\"40\">'\n",
    "        + f'<div class=\"text-muted small text-nowrap mt-2\">{datetime.now().strftime(\"%H:%M:%S\")}</div></div>'\n",
    "        + '<div class=\"flex-shrink-1 bg-light rounded py-2 px-3 ml-3\">'\n",
    "        + f'<div class=\"font-weight-bold mb-1\">You</div>{question}</div>'\n",
    "    )\n",
    "\n",
    "    # Display formatted question\n",
    "    output.append_display_data(HTML(q))\n",
    "\n",
    "    try:\n",
    "        response = qa({\"question\": f\"{question}\", \"chat_history\": chat_history})\n",
    "        answer = response[\"answer\"]\n",
    "        chat_history.append((question, answer))\n",
    "    except Exception as e:\n",
    "        answer = \"<b>Error:</b> \" + str(e)\n",
    "\n",
    "    # Formatting answer for output\n",
    "    # Replacing all $ otherwise matjax would format them in a strange way\n",
    "    answer_formatted = answer.replace('$', r'\\$')\n",
    "    a = (\n",
    "        f'<div class=\"chat-message-left pb-4\"><div>'\n",
    "        + f'<img src=\"assets-resources/cat.png\" class=\"rounded-circle mr-1\" width=\"40\" height=\"40\">'\n",
    "        + f'<div class=\"text-muted small text-nowrap mt-2\">{datetime.now().strftime(\"%H:%M:%S\")}</div></div>'\n",
    "        + '<div class=\"flex-shrink-1 bg-light rounded py-2 px-3 ml-3\">'\n",
    "        + f'<div class=\"font-weight-bold mb-1\">LLM</div>{answer_formatted}</div>'\n",
    "    )\n",
    "\n",
    "    # Turn off loading animation\n",
    "    loading_bar.layout.display = \"none\"\n",
    "\n",
    "    output.append_display_data(HTML(a))\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(cell_1.outputs[0].data[\"text/plain\"].strip(\"'\"))\n",
    "txt_docs = loader.load_and_split()\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "txt_docsearch = Chroma.from_documents(txt_docs, embeddings)\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0.0)\n",
    "\n",
    "chat_history = []\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=txt_docsearch.as_retriever())\n",
    "chat_history = []\n",
    "\n",
    "in_text = widgets.Text()\n",
    "in_text.continuous_update = False\n",
    "in_text.observe(text_eventhandler, \"value\")\n",
    "output = widgets.Output()\n",
    "\n",
    "file = open(\"./assets-resources/loading.gif\", \"rb\")\n",
    "image = file.read()\n",
    "# give a title to the chatbot\n",
    "title = widgets.HTML(\n",
    "    value=\"Q&A with ImageNet Paper\",\n",
    ")\n",
    "loading_bar = widgets.Image(\n",
    "    value=image, format=\"gif\", width=\"20\", height=\"20\", layout={\"display\": \"None\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da3aaaf26234150983142bb0911c6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(),), layout=Layout(display='inline-flex', flex_flow='column-reverse', max_height='500px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52da663fcb964b5e8cb387db3202e030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(HTML(value='Q&A with ImageNet Paper'), Image(value=b'GIF89a\\xc8\\x00\\xc8\\x00\\xf7\\x00\\x00;Ch\\x83\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    widgets.HBox(\n",
    "        [output],\n",
    "        layout=widgets.Layout(\n",
    "            width=\"100%\",\n",
    "            max_height=\"500px\",\n",
    "            display=\"inline-flex\",\n",
    "            flex_flow=\"column-reverse\",\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "display(\n",
    "    widgets.Box(\n",
    "        children=[title, loading_bar, in_text],\n",
    "        layout=widgets.Layout(display=\"flex\", flex_flow=\"row\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples for concept:  Convolutional neural network\n",
      "Image classification: A convolutional neural network can be used to classify images into different categories, such as identifying whether an image contains a cat or a dog.\n",
      "Object detection: CNNs can be used to detect and locate objects in an image, such as identifying the location of cars in a traffic scene.\n",
      "Facial recognition: CNNs can be trained to recognize and distinguish different faces in images or video footage.\n",
      "Medical image analysis: CNNs can be used to analyze medical images, such as identifying tumors or anomalies in X-ray or MRI scans.\n",
      "Natural language processing: CNNs can be applied to analyze and process text data, such as sentiment analysis or text classification tasks.\n",
      "Art generation: CNNs can be used to generate new art pieces, such as generating realistic images from abstract inputs.\n",
      "Autonomous driving: CNNs can be used in self-driving cars to analyze and interpret sensor data, such as identifying pedestrians, traffic signs, and road obstacles.\n",
      "Video classification: CNNs can be used to classify videos into different categories, such as identifying whether a video clip represents a sports event or a news segment.\n"
     ]
    }
   ],
   "source": [
    "from examples import *\n",
    "\n",
    "display_examples(\"Convolutional neural network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz Me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Arial;\"><h3 style=\"color: green;\">Implementation of a convolutional neural network in Pytorch for object detection</h3>\n",
       "            <div>\n",
       "                <input type=\"checkbox\" id=\"exercise\" name=\"exercise\">\n",
       "                <label for=\"exercise\"><b>Question:</b> What is a convolutional neural network (CNN)?</label>\n",
       "                <button onclick=\"document.getElementById('answer_What is a convolutional neural network (CNN)?').style.display = 'block';\">Show Answer</button>\n",
       "                <pre id=\"answer_What is a convolutional neural network (CNN)?\" style=\"display: none;\"><b>Answer:</b> A convolutional neural network (CNN) is a deep learning algorithm that can automatically learn and extract features from input images.</pre>\n",
       "                <button onclick=\"document.getElementById('answer_What is a convolutional neural network (CNN)?').style.display = 'none';\">Hide Answer</button>\n",
       "            </div>\n",
       "            <hr>\n",
       "        \n",
       "            <div>\n",
       "                <input type=\"checkbox\" id=\"exercise\" name=\"exercise\">\n",
       "                <label for=\"exercise\"><b>Question:</b> Why is CNN commonly used for object detection?</label>\n",
       "                <button onclick=\"document.getElementById('answer_Why is CNN commonly used for object detection?').style.display = 'block';\">Show Answer</button>\n",
       "                <pre id=\"answer_Why is CNN commonly used for object detection?\" style=\"display: none;\"><b>Answer:</b> CNN is commonly used for object detection because it can capture spatial dependencies across the image and detect objects with high accuracy.</pre>\n",
       "                <button onclick=\"document.getElementById('answer_Why is CNN commonly used for object detection?').style.display = 'none';\">Hide Answer</button>\n",
       "            </div>\n",
       "            <hr>\n",
       "        \n",
       "            <div>\n",
       "                <input type=\"checkbox\" id=\"exercise\" name=\"exercise\">\n",
       "                <label for=\"exercise\"><b>Question:</b> Explain the concept of object detection using a CNN.</label>\n",
       "                <button onclick=\"document.getElementById('answer_Explain the concept of object detection using a CNN.').style.display = 'block';\">Show Answer</button>\n",
       "                <pre id=\"answer_Explain the concept of object detection using a CNN.\" style=\"display: none;\"><b>Answer:</b> Object detection using a CNN involves feeding an input image into the network, which then convolves the image with several filters to extract features. The network then classifies and localizes objects in the image based on these extracted features.</pre>\n",
       "                <button onclick=\"document.getElementById('answer_Explain the concept of object detection using a CNN.').style.display = 'none';\">Hide Answer</button>\n",
       "            </div>\n",
       "            <hr>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from examples import *\n",
    "from quiz_me import *\n",
    "\n",
    "\n",
    "example = \"Implementation of a convolutional neural network in Pytorch for object detection\"\n",
    "quiz_me(example, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact: convolutional neural network (CNN)\n",
      "Definition: A convolutional neural network (CNN) is a type of deep learning algorithm that is particularly effective for image recognition and processing. It is inspired by the organization of the animal visual cortex, in which neurons in different layers respond to different aspects of the visual field. CNNs are designed to automatically and adaptively learn spatial hierarchies of features from input images.\n",
      "*\n",
      "Fact: deep learning algorithm\n",
      "Definition: Deep learning algorithms are a subset of machine learning algorithms that are inspired by the structure and function of the brain, specifically the neural networks that form the basis of the brain's ability to process and analyze complex information. These algorithms are typically used for tasks that require large-scale data analysis and pattern recognition, such as image and speech recognition, natural language processing, and autonomous driving.\n",
      "*\n",
      "Fact: learn and extract features\n",
      "Definition: Learning and extracting features refers to the process of analyzing input data (such as images, text, or audio) and identifying relevant patterns or characteristics that can be used to classify or understand the data. In the context of a CNN, this involves the network automatically learning and extracting visual features from input images, such as edges, textures, shapes, and objects.\n",
      "*\n",
      "Fact: object detection\n",
      "Definition: Object detection is a computer vision task that involves identifying and locating objects of interest in digital images or videos. It is a fundamental task in many applications, such as autonomous driving, surveillance, and image search. CNNs are commonly used for object detection because they are able to capture spatial dependencies across an image and learn discriminative features that can be used to detect objects with high accuracy.\n",
      "*\n",
      "Fact: capture spatial dependencies\n",
      "Definition: Spatial dependencies refer to the relationships and dependencies between different parts of an image or other spatial data. In the context of object detection, capturing spatial dependencies means that the CNN is able to take into account the context and relative positions of objects in the image, which can improve the accuracy of the object detection task.\n",
      "*\n",
      "Fact: high accuracy\n",
      "Definition: High accuracy refers to the ability of a model or algorithm to make correct predictions or classifications with a high degree of certainty. In the context of object detection using CNNs, high accuracy means that the network is able to accurately identify and locate objects in an image, minimizing false positives and false negatives.\n",
      "*\n",
      "Fact: input image\n",
      "Definition: An input image is the initial data or image that is fed into a CNN for analysis or processing. The input image can be a digital photograph, a frame from a video, or any other type of digital image. The CNN convolves the input image with a series of filters to extract visual features, which are then used for object detection or other tasks.\n",
      "*\n",
      "Fact: network\n",
      "Definition: A network refers to the architecture or structure of a convolutional neural network. It consists of multiple layers of interconnected nodes, or neurons, which process and transform the input data to produce an output. In the context of object detection using CNNs, the network is designed to extract and learn relevant features from input images and use them to classify and locate objects.\n",
      "*\n",
      "Fact: convolves the image\n",
      "Definition: Convolving the image refers to the process of applying a convolutional filter to the input image. The filter is a small matrix of weights that is applied to each pixel of the image, and the convolution operation combines the filter and the image to produce a feature map. This process helps the CNN extract relevant visual features from the input image.\n",
      "*\n",
      "Fact: several filters\n",
      "Definition: Filters, also known as kernels or convolutional filters, are small matrices of weights that are used in the convolutional layer of a CNN. Each filter is designed to extract a specific feature or pattern from the input image, such as an edge, a texture, or a shape. By applying multiple filters to an input image, the CNN is able to extract a diverse set of visual features.\n",
      "*\n",
      "Fact: classifies and localizes objects\n",
      "Definition: Classifying and localizing objects refers to the task of identifying and categorizing objects in an image, as well as determining their precise locations or bounding boxes. In the context of object detection using a CNN, the network uses the extracted features to classify objects into different categories (e.g., person, car, cat) and determine the positions and sizes of the bounding boxes that enclose the objects.\n",
      "*\n",
      "Fact: extracted features\n",
      "Definition: Extracted features are the visual characteristics or patterns that are identified and extracted from an input image by a CNN. These features can include edges, textures, shapes, and higher-level visual representations that are relevant for the task at hand, such as object recognition or classification. The extracted features can be used for various purposes, such as detecting and localizing objects, analyzing image content, or generating descriptive captions.\n",
      "*\n"
     ]
    }
   ],
   "source": [
    "from info_extraction import *\n",
    "\n",
    "prompt = \"\"\"\n",
    "Answer: A convolutional neural network (CNN) is a deep learning algorithm that can automatically learn and extract features from input images.\n",
    "\n",
    "Answer: CNN is commonly used for object detection because it can capture spatial dependencies across the image and detect objects with high accuracy.\n",
    "\n",
    "Answer: Object detection using a CNN involves feeding an input image into the network, which then convolves the image with several filters to extract features. The network then classifies and localizes objects in the image based on these extracted features.\n",
    "\n",
    "\"\"\"\n",
    "display_info(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anki Flashcards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clozed deletions: 6\n",
      "A {c1::convolutional neural network (CNN)} is a type of {c1::deep learning algorithm} that is particularly effective for {c1::image recognition} and {c1::processing}. It is inspired by the organization of the animal visual cortex, in which neurons in different layers respond to different aspects of the visual field. {c1::CNNs} are designed to automatically and adaptively learn {c1::spatial hierarchies of features} from input images.\n"
     ]
    }
   ],
   "source": [
    "from flashcards import *\n",
    "\n",
    "basic_card = \"\"\"\n",
    "A convolutional neural network (CNN) is a type of deep learning algorithm that is particularly effective for image recognition and processing. It is inspired by the organization of the animal visual cortex, in which neurons in different layers respond to different aspects of the visual field. CNNs are designed to automatically and adaptively learn spatial hierarchies of features from input images.\n",
    "\"\"\"\n",
    "response = create_flashcard(basic_card)\n",
    "print(f\"Number of clozed deletions: {response.num_of_clozed_deletions}\")\n",
    "print(response.text_with_cloze_deletions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A with Validated Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Answer:</h3><p><b>Fact:</b> The author grew up in Sao Paulo Brazil.</p><ul><li> <p><b>source:</b> Sao Paulo Brazil</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: instructor docs!\n",
    "from citation_validation import *\n",
    "\n",
    "question = \"What did the author grew up?\"\n",
    "context = \"\"\"\n",
    "My name is Lucas soares, and I grew up in Sao Paulo Brazil but I moved to Portugal 6 years ago.\n",
    "I went to a science high school but in university I studied Artificial Intelligence.\n",
    "I worked in a research lab called Champalimaud Institute working on AI applications to Neuroscience.\n",
    "I also love training jiu jitsu for which I earned a black belt last year after 10 years of training.\n",
    "\"\"\"\n",
    "\n",
    "query_validate(question, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"1767pt\" height=\"487pt\"\n",
       " viewBox=\"0.00 0.00 1766.75 486.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 482.5)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-482.5 1762.75,-482.5 1762.75,4 -4,4\"/>\n",
       "<!-- 1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"#ffce54\" cx=\"444.72\" cy=\"-460.5\" rx=\"167.02\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"444.72\" y=\"-455.45\" font-family=\"Times,serif\" font-size=\"14.00\">Fundamentals of Large Language Models</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"#ffce54\" cx=\"405.72\" cy=\"-372\" rx=\"119.93\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"405.72\" y=\"-366.95\" font-family=\"Times,serif\" font-size=\"14.00\">Natural Language Processing</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M437.02,-442.41C431.66,-430.52 424.39,-414.41 418.21,-400.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"421.47,-399.41 414.17,-391.73 415.09,-402.28 421.47,-399.41\"/>\n",
       "<text text-anchor=\"middle\" x=\"456.85\" y=\"-411.2\" font-family=\"Times,serif\" font-size=\"14.00\">belongs to</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"#ffce54\" cx=\"622.72\" cy=\"-372\" rx=\"79.5\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"622.72\" y=\"-366.95\" font-family=\"Times,serif\" font-size=\"14.00\">Machine Learning</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M479.46,-442.62C508.28,-428.61 549.36,-408.65 580.15,-393.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"581.34,-397 588.81,-389.48 578.28,-390.7 581.34,-397\"/>\n",
       "<text text-anchor=\"middle\" x=\"580.1\" y=\"-411.2\" font-family=\"Times,serif\" font-size=\"14.00\">is based on</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"#ffce54\" cx=\"622.72\" cy=\"-283.5\" rx=\"66.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"622.72\" y=\"-278.45\" font-family=\"Times,serif\" font-size=\"14.00\">Deep Learning</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M622.72,-353.91C622.72,-342.26 622.72,-326.55 622.72,-313.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"626.22,-313.36 622.72,-303.36 619.22,-313.36 626.22,-313.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"663.22\" y=\"-322.7\" font-family=\"Times,serif\" font-size=\"14.00\">is a subfield of</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"#a0d468\" cx=\"213.72\" cy=\"-195\" rx=\"105.6\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.72\" y=\"-189.95\" font-family=\"Times,serif\" font-size=\"14.00\">Transformer Architecture</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M571.75,-271.72C500.71,-256.7 371.41,-229.35 289.18,-211.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"290.14,-208.58 279.63,-209.94 288.69,-215.43 290.14,-208.58\"/>\n",
       "<text text-anchor=\"middle\" x=\"475.22\" y=\"-234.2\" font-family=\"Times,serif\" font-size=\"14.00\">utilizes</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"#a0d468\" cx=\"419.72\" cy=\"-195\" rx=\"54.93\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"419.72\" y=\"-189.95\" font-family=\"Times,serif\" font-size=\"14.00\">Pre&#45;training</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;6 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M586.2,-268.14C570.6,-261.92 552.22,-254.48 535.72,-247.5 510.78,-236.95 483.06,-224.66 460.97,-214.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"462.55,-211.61 451.99,-210.69 459.67,-217.99 462.55,-211.61\"/>\n",
       "<text text-anchor=\"middle\" x=\"555.22\" y=\"-234.2\" font-family=\"Times,serif\" font-size=\"14.00\">utilizes</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>7</title>\n",
       "<ellipse fill=\"none\" stroke=\"#a0d468\" cx=\"546.72\" cy=\"-195\" rx=\"53.91\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"546.72\" y=\"-189.95\" font-family=\"Times,serif\" font-size=\"14.00\">Fine&#45;tuning</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;7 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M608.07,-265.82C596.92,-253.14 581.4,-235.47 568.76,-221.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"571.56,-218.97 562.33,-213.76 566.3,-223.59 571.56,-218.97\"/>\n",
       "<text text-anchor=\"middle\" x=\"611.22\" y=\"-234.2\" font-family=\"Times,serif\" font-size=\"14.00\">utilizes</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>8</title>\n",
       "<ellipse fill=\"none\" stroke=\"#a0d468\" cx=\"787.72\" cy=\"-195\" rx=\"169.07\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"787.72\" y=\"-189.95\" font-family=\"Times,serif\" font-size=\"14.00\">GPT (Generative Pre&#45;trained Transformer)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;8 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>4&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M652.22,-267.04C678.09,-253.47 716.02,-233.59 745.17,-218.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"746.51,-221.55 753.75,-213.81 743.26,-215.36 746.51,-221.55\"/>\n",
       "<text text-anchor=\"middle\" x=\"743.22\" y=\"-234.2\" font-family=\"Times,serif\" font-size=\"14.00\">includes</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>9</title>\n",
       "<ellipse fill=\"none\" stroke=\"#a0d468\" cx=\"1233.72\" cy=\"-195\" rx=\"258.63\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1233.72\" y=\"-189.95\" font-family=\"Times,serif\" font-size=\"14.00\">BERT (Bidirectional Encoder Representations from Transformers)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;9 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>4&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M680.48,-274.32C778.83,-260.4 980.85,-231.8 1111.64,-213.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1111.98,-216.77 1121.39,-211.9 1111,-209.84 1111.98,-216.77\"/>\n",
       "<text text-anchor=\"middle\" x=\"1006.22\" y=\"-234.2\" font-family=\"Times,serif\" font-size=\"14.00\">includes</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>10</title>\n",
       "<ellipse fill=\"none\" stroke=\"#a0d468\" cx=\"1634.72\" cy=\"-195\" rx=\"124.03\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1634.72\" y=\"-189.95\" font-family=\"Times,serif\" font-size=\"14.00\">Encoder&#45;Decoder Architecture</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;10 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>4&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M686.79,-278.5C831.99,-269.19 1197.18,-244.48 1501.72,-213 1512.49,-211.89 1523.73,-210.62 1534.92,-209.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1535.07,-212.79 1544.58,-208.12 1534.23,-205.84 1535.07,-212.79\"/>\n",
       "<text text-anchor=\"middle\" x=\"1332.22\" y=\"-234.2\" font-family=\"Times,serif\" font-size=\"14.00\">includes</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>11</title>\n",
       "<ellipse fill=\"none\" stroke=\"#a0d468\" cx=\"122.72\" cy=\"-106.5\" rx=\"92.3\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"122.72\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">Attention Mechanism</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;11 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>5&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M195.74,-176.91C182.28,-164.12 163.67,-146.43 148.61,-132.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"151.42,-129.95 141.76,-125.6 146.6,-135.02 151.42,-129.95\"/>\n",
       "<text text-anchor=\"middle\" x=\"199.97\" y=\"-145.7\" font-family=\"Times,serif\" font-size=\"14.00\">employs</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>12</title>\n",
       "<ellipse fill=\"none\" stroke=\"#a0d468\" cx=\"356.72\" cy=\"-106.5\" rx=\"123.52\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"356.72\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">Contextual Word Embeddings</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;12 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>6&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M407.57,-177.32C398.63,-165.04 386.28,-148.08 375.99,-133.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"378.87,-131.97 370.15,-125.94 373.21,-136.09 378.87,-131.97\"/>\n",
       "<text text-anchor=\"middle\" x=\"414.22\" y=\"-145.7\" font-family=\"Times,serif\" font-size=\"14.00\">utilizes</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>13</title>\n",
       "<ellipse fill=\"none\" stroke=\"#a0d468\" cx=\"585.72\" cy=\"-106.5\" rx=\"87.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"585.72\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">Large&#45;scale Datasets</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;13 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>6&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M447.08,-179.18C466.35,-168.79 492.79,-154.65 516.22,-142.5 524.93,-137.99 534.28,-133.22 543.22,-128.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"544.75,-131.85 552.11,-124.23 541.6,-125.6 544.75,-131.85\"/>\n",
       "<text text-anchor=\"middle\" x=\"538.47\" y=\"-145.7\" font-family=\"Times,serif\" font-size=\"14.00\">requires</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>7&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M554.43,-176.91C559.79,-165.02 567.05,-148.91 573.23,-135.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"576.36,-136.78 577.28,-126.23 569.98,-133.91 576.36,-136.78\"/>\n",
       "<text text-anchor=\"middle\" x=\"591.47\" y=\"-145.7\" font-family=\"Times,serif\" font-size=\"14.00\">requires</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>14</title>\n",
       "<ellipse fill=\"none\" stroke=\"#a0d468\" cx=\"1233.72\" cy=\"-106.5\" rx=\"119.42\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1233.72\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">Domain&#45;Specific Knowledge</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>8&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M866.69,-178.68C946.74,-163.16 1070.07,-139.24 1151.33,-123.48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1151.87,-126.94 1161.02,-121.6 1150.54,-120.07 1151.87,-126.94\"/>\n",
       "<text text-anchor=\"middle\" x=\"1084.1\" y=\"-145.7\" font-family=\"Times,serif\" font-size=\"14.00\">incorporates</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;14 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>9&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1233.72,-176.91C1233.72,-165.26 1233.72,-149.55 1233.72,-136.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1237.22,-136.36 1233.72,-126.36 1230.22,-136.36 1237.22,-136.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"1267.1\" y=\"-145.7\" font-family=\"Times,serif\" font-size=\"14.00\">incorporates</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;14 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>10&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1567.6,-179.52C1496.88,-164.27 1385.63,-140.27 1311.22,-124.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1312.05,-120.82 1301.54,-122.13 1310.57,-127.66 1312.05,-120.82\"/>\n",
       "<text text-anchor=\"middle\" x=\"1504.1\" y=\"-145.7\" font-family=\"Times,serif\" font-size=\"14.00\">incorporates</text>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>15</title>\n",
       "<ellipse fill=\"none\" stroke=\"#a0d468\" cx=\"45.72\" cy=\"-18\" rx=\"45.72\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"45.72\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">Inference</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;15 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>12&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M290.75,-90.92C265.18,-84.99 235.74,-77.81 209.22,-70.5 168.62,-59.31 122.95,-44.75 90.22,-33.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"91.67,-30.77 81.08,-30.95 89.47,-37.41 91.67,-30.77\"/>\n",
       "<text text-anchor=\"middle\" x=\"229.97\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">enables</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>16</title>\n",
       "<ellipse fill=\"none\" stroke=\"#a0d468\" cx=\"180.72\" cy=\"-18\" rx=\"71.31\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"180.72\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">Text Generation</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;16 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>12&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M322.79,-88.82C294.07,-74.71 252.81,-54.43 222.15,-39.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"224.06,-36.4 213.55,-35.13 220.98,-42.68 224.06,-36.4\"/>\n",
       "<text text-anchor=\"middle\" x=\"304.97\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">enables</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>17</title>\n",
       "<ellipse fill=\"none\" stroke=\"#a0d468\" cx=\"356.72\" cy=\"-18\" rx=\"86.67\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"356.72\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">Question Answering</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;17 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>12&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M356.72,-88.41C356.72,-76.76 356.72,-61.05 356.72,-47.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"360.22,-47.86 356.72,-37.86 353.22,-47.86 360.22,-47.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"376.97\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">enables</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>18</title>\n",
       "<ellipse fill=\"none\" stroke=\"#a0d468\" cx=\"548.72\" cy=\"-18\" rx=\"87.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"548.72\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">Machine Translation</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;18 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>12&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M393.29,-89.03C424.51,-74.96 469.55,-54.67 503.12,-39.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"504.53,-42.75 512.21,-35.45 501.65,-36.37 504.53,-42.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"490.97\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">enables</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>19</title>\n",
       "<ellipse fill=\"none\" stroke=\"#a0d468\" cx=\"758.72\" cy=\"-18\" rx=\"104.07\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"758.72\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">Language Understanding</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;19 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>12&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M424.01,-91.02C495.87,-75.56 609.45,-51.12 684.03,-35.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"684.68,-38.51 693.72,-32.99 683.21,-31.67 684.68,-38.51\"/>\n",
       "<text text-anchor=\"middle\" x=\"614.97\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">enables</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x14d900af0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge graph generated and visualized.\n"
     ]
    }
   ],
   "source": [
    "# source: pydantic presentation by Jason Liu\n",
    "from visualize_knowledge_graph import knowledge_graph_primitive\n",
    "\n",
    "knowledge_graph_primitive(\"the Fundamentals of Large Language Models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-talk",
   "language": "python",
   "name": "oreilly-talk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
